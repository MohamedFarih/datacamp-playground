{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Inheriting the Spider\n",
    "When learning about scrapy spiders, we saw that the main portion of the code for us to adjust is the class for the spider. To help build some familiarity of the class, you will complete a short piece of code to complete a toy-model of the spider class code. We've omitted the code that would actually run the spider, only including the pieces necessary to create the class.\n",
    "\n",
    "As mentioned in the lesson, a class is roughly a collection of related variables and functions housed together. Sometimes one class likes to use methods from another class, and so we will inherit methods from a different class. That's what we do in the spider class.\n",
    "\n",
    "We wrote the function inspect_class to look at the your class once you're done, if you'd like to test your solution!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your spider class name is: your_spider\nIt seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "def inspect_class(c):\n",
    "  newc = c()\n",
    "  meths = dir(newc)\n",
    "  if 'name' in meths:\n",
    "    print(\"Your spider class name is:\", newc.name)\n",
    "  if 'from_crawler' in meths:\n",
    "    print(\"It seems you have inherited methods from scrapy.Spider -- NICE!\")\n",
    "  else:\n",
    "    print(\"Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!\")\n",
    "    \n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    pass\n",
    "  # parse method\n",
    "  def parse(self, response):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "source": [
    "# Hurl the URLs\n",
    "In the next lesson we will talk about the start_requests method within the spider class. In this quick exercise, we ask you to change around a variable within the start_requests method which foreshadows some of what we will be learning in the next lesson. Basically, we want you to start becoming comfortable turning some of the wheels within a spider class; in this case, making a list of urls within the start_requests method.\n",
    "\n",
    "We've written a function inspect_class which will print out the list of elements you have in the urls variable within the start_requests method.\n",
    "\n",
    "Note: in the next several exercises, you will write code to complete your spider class, but the code does not yet include the pieces to actually run the spider; that will come at the end."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The start_requests method yields the following urls:\n\t- https://www.datacamp.com\n\t- https://scrapy.org\n"
     ]
    }
   ],
   "source": [
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  meths = dir( newc )\n",
    "  if 'start_requests' in meths:\n",
    "    print( \"The start_requests method yields the following urls:\" )\n",
    "    for u in newc.start_requests():\n",
    "      print(  \"\\t-\", u )\n",
    "    \n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "      yield url\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "source": [
    "# Self Referencing is Classy\n",
    "You probably have noticed that within the spider class, we always input the argument self in the start_requests and parse methods (just look in the sample code in this exercise!). This allows us to reference between methods within the class. That is, if we want to refer to the method parse within the start_requests method, we would need to write self.parse rather than just parse; what writing self does is tell the code: \"Look in the same class as start_requests for a method called parse to use.\"\n",
    "\n",
    "In this exercise you will get a chance to play with this \"self referencing\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Calling start_requests in YourSpider prints out: Hello World!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  try:\n",
    "    newc.start_requests()\n",
    "  except:\n",
    "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    self.print_msg( \"Hello World!\" )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  # print_msg method\n",
    "  def print_msg( self, msg ):\n",
    "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "source": [
    "# Starting with Start Requests\n",
    "In the last lesson we learned about setting up the start_requests method within a scrapy spider. Here we have another toy-model spider which doesn't actually scrape anything, but gives you a chance to play with the start_requests method. What we want is for you to start becomming familiar with the arguments you pass into the scrapy.Request call within start_requests.\n",
    "\n",
    "As before, we have created the function inspect_class to examine what you are yielding in start_requests."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The url you would scrape is: https://www.datacamp.com\nThe name of the callback method you called is: parse\n"
     ]
    }
   ],
   "source": [
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  try:\n",
    "    y = list( newc.start_requests() )\n",
    "    first_yield = y[0]\n",
    "    print( \"The url you would scrape is:\", first_yield.url )\n",
    "    cb = first_yield.callback\n",
    "    print( \"The name of the callback method you called is:\", cb.__name__ )\n",
    "  except:\n",
    "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url= \"https://www.datacamp.com\", callback=self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "source": [
    "# Pen Names\n",
    "In this exercise, we have set up a spider class which, when finished, will retrieve the author names from a shortened version of the DataCamp course directory. The URL for the shortened version is stored in the variable url_short. Your job will be to create the list of extracted author names in the parse method of the spider.\n",
    "\n",
    "Two things you should know:\n",
    "\n",
    "- You will be using the response object and the css method here.\n",
    "- The course author names are defined by the text within the paragraph p elements belonging to the class course-block__author-name\n",
    "\n",
    "You can inspect the spider using the function inspect_spider() that we built for you -- it will print out the author names you find!\n",
    "\n",
    "Note that this and the remaining exercises in this chapter may take some time to load."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Oh no! Something went wrong with the code. Keep trying!\n"
     ]
    }
   ],
   "source": [
    "def inspect_spider( s ):\n",
    "  news = s()\n",
    "  try:\n",
    "    req = list( news.start_requests() )[0]\n",
    "    url = req.url\n",
    "    html = requests.get( url ).content\n",
    "    response = TextResponse( url = url, body = html, encoding = 'utf-8' )\n",
    "    author_names = req.callback( response )\n",
    "    print( 'You have collected the author names:')\n",
    "    for a in author_names:\n",
    "      print('\\t-', a )\n",
    "  except:\n",
    "    print( 'Oh no! Something went wrong with the code. Keep trying!')\n",
    "\n",
    "url_short = \"https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short\"\n",
    "\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = 'dcspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    # Create an extracted list of course author names\n",
    "    author_names = response.css('p.course-block__author-name::text').extract()\n",
    "    # Here we will just return the list of Authors\n",
    "    return author_names\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "source": [
    "# Crawler Time\n",
    "This will be your first chance to play with a spider which will crawl between sites (by first collecting links from one site, and following those links to parse new sites). This spider starts at the shortened DataCamp course directory, then extracts the links of the courses in the parse method; from there, it will follow those links to extract the course descriptions from each course page in the parse_descr method, and put these descriptions into the list course_descrs. Your job is to complete the code so that the spider runs as desired!\n",
    "\n",
    "We have created a function inspect_spider which will print out one of the course descriptions you scrape (if done correctly)!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Oh no! Something is wrong with the code. Keep trying!\n"
     ]
    }
   ],
   "source": [
    "def inspect_spider( s ):\n",
    "  news = s()\n",
    "  try:\n",
    "    req1 = list( news.start_requests() )[0]\n",
    "    html1 = requests.get( req1.url ).content\n",
    "    response1 = TextResponse( url = req1.url, body = html1, encoding = 'utf-8' )\n",
    "    req2 = list( news.parse( response1 ) )[0]\n",
    "    html2 = requests.get( req2.url ).content\n",
    "    response2 = TextResponse( url = req2.url, body = html2, encoding = 'utf-8' )\n",
    "    for d in news.parse_descr( response2 ):\n",
    "      print(\"One course description you found is:\", d )\n",
    "      break\n",
    "  except:\n",
    "    print(\"Oh no! Something is wrong with the code. Keep trying!\")\n",
    "\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow(url= link, callback = self.parse_descr)\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "source": [
    "# Time to Run\n",
    "In the last lesson, we went through creating an entire web-crawler to access course information from each course in the DataCamp course directory. However, the lesson seemed to stop without a climax, because we didn't play with the code after finishing the parsing methods.\n",
    "\n",
    "The point of this exercise is to remedy that!\n",
    "\n",
    "The code we give you to look at in this and the next exercise is long, because its the entire spider that took us the lesson to create! However, don't be intimidated! The point of these two exercises is to give you a very easy task to complete, with the hope that you will look at and run the code for this spider. That way, even though it is long, you will have a grasp of it!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "def previewCourses( dc_dict, n = 3 ):\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    for i,ct in enumerate(dc_dict[t]):\n",
    "      print(\"\\tChapter %d: %s\" % (i+1,ct) )\n",
    "    print(\"\")\n",
    "    \n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCamp Descriptions\n",
    "Like the previous exercise, the code here is long since you are working with an entire web-crawling spider! But again, don't let the amount of code intimidate you, you have a handle on how spiders work now, and you are perfectly capable to complete the easy task for you here!\n",
    "\n",
    "As in the previous exercise, we have created a function previewCourses which lets you preview the output of the spider, but you can always just explore the dictionary dc_dict too after you run the code.\n",
    "\n",
    "In this exercise, you are asked to create a CSS Locator string direct to the text of the course description. All you need to know is that from the course page, the course description text is within a paragraph p element which belongs to the class course__description (two underlines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css( 'p.course__description::text' )\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "source": [
    "# Capstone Crawler\n",
    "This exercise gives you a chance to show off what you've learned! In this exercise, you will write the parse function for a spider and then fill in a few blanks to finish off the spider. On the course directory page of DataCamp, each listed course has a title and a short course description. This spider will be used to scrape the course directory to extract the course titles and short course descriptions. You will not need to follow any links this time. Everything you need to know is:\n",
    "\n",
    "The course titles are defined by the text within an h4 element whose class contains the string block__title (double underline).\n",
    "The short course descriptions are defined by the text within a paragraph p element whose class contains the string block__description (double underline)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse method\n",
    "def parse(self, response):\n",
    "  # Extracted course titles\n",
    "  crs_titles = response.xpath('//h4[contains(@class,\\\"block__title\\\")]/text()').extract()\n",
    "  # Extracted course descriptions\n",
    "  crs_descrs = response.xpath('//p[contains(@class,\\\"block__description\\\")]/text()').extract()\n",
    "  # Fill in the dictionary: it is the spider output\n",
    "  for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "    dc_dict[crs_title] = crs_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = 'yourspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request(url = url_short, callback = self.parse)\n",
    "      \n",
    "  def parse(self, response):\n",
    "    # My version of the parser you wrote in the previous part\n",
    "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):\n",
    "      dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  }
 ]
}