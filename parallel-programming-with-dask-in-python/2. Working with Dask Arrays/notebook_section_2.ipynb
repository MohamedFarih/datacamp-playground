{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Section 2: Working with Dask Arrays"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Inspecting a Dask array\n",
    "A dask.array called energy is provided for you. The array energy contains electricity load in kWh in Texas sampled every 15 minutes for the year 2000. This dataset was provided by ERCOT.\n",
    "\n",
    "Your job is to determine the correct shape of the array energy and the total electricity (in kWh) consumed by Texas for the year 2000 (i.e., the sum of the electricity load). Remember to leverage both the .sum() and .compute() methods."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = '../datasets/Texas/Texas/texas.2000.hdf5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "\n",
    "import dask.array as da\n",
    "\n",
    "energy = da.from_array(data, chunks=1000)\n",
    "energy.sum().compute()"
   ]
  },
  {
   "source": [
    "# Chunking a NumPy array\n",
    "A NumPy array has been provided for you as energy. This is the electricity load in kWh for the state of Texas sampled every 15 minutes over the year 2000 (that's about 35 thousand samples).\n",
    "\n",
    "Your job is to convert the NumPy array into a dask.array; the dask.array should have chunks whose sizes are 1/4 of the number of elements of the array energy. You will then inspect the chunk sizes of the Dask array. Finally, you'll compute the mean electricity load in kWh in two ways (using the dask.array and using the NumPy array) to compare the results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "filename = '../datasets/Texas/Texas/texas.2000.hdf5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    energy = np.array(f[a_group_key])\n",
    "\n",
    "# Call da.from_array():  energy_dask\n",
    "energy_dask = da.from_array(energy, chunks=len(energy)/4)\n",
    "\n",
    "# Print energy_dask.chunks\n",
    "print(energy_dask.chunks)\n",
    "\n",
    "# Print Dask array average and then NumPy array average\n",
    "print(energy_dask.mean().compute())\n",
    "print(energy.mean())"
   ]
  },
  {
   "source": [
    "# Timing Dask array computations\n",
    "Your job now is to create two Dask arrays from energy using different chunksizes. You'll then measure the time required (in milliseconds) to compute the standard deviation of each Dask array.\n",
    "\n",
    "The NumPy array energy is provided as before and the module dask.array is imported for you as da."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = '../datasets/Texas/Texas/texas.2000.hdf5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import time\n",
    "import time\n",
    "\n",
    "# Call da.from_array() with arr: energy_dask4\n",
    "energy_dask4 = da.from_array(energy, chunks=len(energy)/4)\n",
    "\n",
    "# Print the time to compute standard deviation\n",
    "t_start = time.time()\n",
    "std_4 = energy_dask4.std().compute()\n",
    "t_end = time.time()\n",
    "print((t_end - t_start) * 1.0e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import time\n",
    "import time\n",
    "\n",
    "# Call da.from_array() with arr: energy_dask8\n",
    "energy_dask8 = da.from_array(energy, chunks=len(energy)/8)\n",
    "\n",
    "# Print the time to compute standard deviation\n",
    "t_start = time.time()\n",
    "std_8 = energy_dask8.std().compute()\n",
    "t_end = time.time()\n",
    "print((t_end - t_start) * 1.0e3)"
   ]
  },
  {
   "source": [
    "# Predicting result of broadcasting\n",
    "Binary arithmetic operations between two arrays are possible only when their shapes are compatible. Being compatible means either that the two arrays have identical .shape attributes or that their shapes satisfy certain rules. The rules for broadcasting in NumPy are summarized in the documentation.\n",
    "\n",
    "Here, the NumPy arrays a, b, c, and d are provided for you. They differ in shape, number of dimensions, or in the extent of each dimension. Which of the possible sums listed below yields an array with shape (10,10)?\n",
    "\n",
    "```\n",
    "In [1]: \n",
    "a.shape\n",
    "Out[1]:\n",
    "()\n",
    "In [2]:\n",
    "b.shape \n",
    "Out[2]:\n",
    "(10, 1, 1)\n",
    "In [3]:\n",
    "c.shape\n",
    "Out[3]:\n",
    "(10, 10)\n",
    "In [4]:\n",
    "d.shape\n",
    "Out[4]:\n",
    "(1, 10)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Subtracting & broadcasting\n",
    "Now that you're comfortable with broadcasting rules, you're going to practice using the .reshape() method together with broadcasting.\n",
    "\n",
    "The one-dimensional array load_2001 holds the total electricity load for the state of Texas sampled every 15 minutes for the entire year 2001 (35040 samples in total). The one-dimensional array load_recent holds the corresponding data sampled for each of the years 2013 through 2015 (i.e., 105120 samples consisting of the samples from 2013, 2014, & 2015 in sequence). None of these years are leap years, so each year has 365 days. Observe also that there are 96 intervals of duration 15 minutes in each day.\n",
    "\n",
    "Your job is to compute the differences of the samples in the years 2013 to 2015 each from the corresponding samples of 2001."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import h5py and dask.array\n",
    "import h5py\n",
    "import dask.array as da\n",
    "import numpy as np \n",
    "\n",
    "filename = '../datasets/Texas/Texas/texas.2001.hdf5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "\n",
    "load_2001 = np.array(data)\n",
    "\n",
    "\n",
    "filename = '../datasets/Texas/Texas/texas.2013.hdf5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "\n",
    "load_2013 = np.array(data)\n",
    "\n",
    "\n",
    "filename = '../datasets/Texas/Texas/texas.2014.hdf5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "\n",
    "load_2014 = np.array(data)\n",
    "\n",
    "filename = '../datasets/Texas/Texas/texas.2015.hdf5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "\n",
    "load_2015 = np.array(data)\n",
    "\n",
    "load_recent = np.array([load_2013, load_2014, load_2015])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape load_recent to three dimensions: load_recent_3d\n",
    "load_recent_3d = load_recent.reshape((3,365,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape load_2001 to three dimensions: load_2001_3d\n",
    "load_2001_3d = load_2001.reshape((1,365,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the load in 2001 from the load in 2013 - 2015: diff_3d\n",
    "diff_3d = load_recent_3d - load_2001_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the difference each year on March 2 at noon\n",
    "print(diff_3d[:, 61, 48])"
   ]
  },
  {
   "source": [
    "# Computing aggregations\n",
    "You'll now compute summary statistics by aggregating NumPy arrays across various dimensions (in this case, giving trends in electricity usage).\n",
    "\n",
    "The load_recent_3d NumPy array from the last exercise is provided for you. It contains the electricity load (in kWh) sampled every 15 minutes from the start of 2013 to the end of 2015. The possible index values of the three-dimensional array correspond to year (from 0 to 2), day (from 0 to 364), and 15-minute interval (from 0 to 95); remember, NumPy arrays are indexed from zero. Thus, load_recent_3d[0,1,2] is the electricity load consumed on January 2nd, 2013 from 00:30:00 AM to 00:45:00 AM."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print maximum of load_recent_3d across 2nd & 3rd dimensions\n",
    "print(load_recent_3d.max(axis=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sum along last dimension of load_recent_3d: daily_consumption\n",
    "daily_consumption = load_recent_3d.sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean of 62nd row of daily_consumption\n",
    "print(daily_consumption[:,61].mean())"
   ]
  },
  {
   "source": [
    "# Reading the data\n",
    "For this exercise, the monthly average maximum temperature (degrees Celsius) data on a 10km grid has been downloaded from NOAA for the continental US.\n",
    "\n",
    "The data is stored in separate HDF5 files from 2008 to 2011. Your job is to use h5py to connect to the datasets and prepare a list of Dask arrays where each element is one year of data. The list of filenames is provided for you as filenames. The chunksizes have been chosen to optimize reading data from disk.\n",
    "\n",
    "For this exercise you'll utilize a list comprehension to quickly iterate through the filenames list and form a new list of h5py file handles and again to make a list of Dask arrays."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import h5py and dask.array\n",
    "import h5py\n",
    "import dask.array as da\n",
    "\n",
    "# List comprehension to read each file: dsets\n",
    "dsets = [h5py.File(f)[\"/tmax\"] for f in filenames]\n",
    "\n",
    "# List comprehension to make dask arrays: monthly\n",
    "monthly = [da.from_array(d, chunks=(1,444,922)) for d in dsets]"
   ]
  },
  {
   "source": [
    "# Stacking data & reading climatology\n",
    "Now that we have a list of Dask arrays, your job is to call da.stack() to make a single Dask array where month and year are in separate dimensions. You'll also read the climatology data set, which is the monthly average max temperature again from 1970 to 2000."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference: diff\n",
    "diff = (by_year - climatology)* 9/5\n",
    "# Compute the average over last two axes: avg\n",
    "avg = da.nanmean(diff, axis=(-1,-2)).compute()\n",
    "# Plot the slices [:,0], [:,7], and [:11] against the x values\n",
    "x = range(2008,2012)\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(x,avg[:,0], label='Jan')\n",
    "ax.plot(x,avg[:,7], label='Aug')\n",
    "ax.plot(x,avg[:,11], label='Dec')\n",
    "ax.axhline(0, color='red')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Difference (degrees Fahrenheit)')\n",
    "ax.legend(loc=0)\n",
    "plt.show()"
   ]
  }
 ]
}