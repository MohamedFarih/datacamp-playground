{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Delaying reading & cleaning\n",
    "To work with this subset of the monthly flight information data efficiently, you'll need to do a bit of cleaning. Specifically, you'll need to replace zeros in the 'WEATHER_DELAY' column with nan. This substitution will make counting delays much easier later. This operation requires you to build a delayed pipeline of pandas DataFrame manipulations. You will then convert the output to a Dask DataFrame in which each file will be one chunk.\n",
    "\n",
    "Your first job is to write a function to read a single CSV file into a DataFrame. The DataFrame returned will use pandas TimeStamps in the 'FL_DATE' column, and will have 0s replaced with np.nans in the 'WEATHER_DELAY' column. You can use the flightdelays-2016-1.csv file to verify that the function works as intended."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "# Define @delayed-function read_flights\n",
    "@delayed\n",
    "def read_flights(filename):\n",
    "\n",
    "    # Read in the DataFrame: df\n",
    "    df = pd.read_csv(filename, parse_dates=['FL_DATE'])\n",
    "\n",
    "    # Replace 0s in df['WEATHER_DELAY'] with np.nan\n",
    "    df['WEATHER_DELAY'] = df['WEATHER_DELAY'].replace(0, np.nan)\n",
    "\n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "source": [
    "# Reading all flight data\n",
    "A list called filenames is provided for you at the start of this exercise; it contains the strings \"flightdelays-2016-1.csv\" through \"flightdelays-2016-12.csv\". In addition, the delayed function read_flights() defined in the last exercise is provided for you. Also, Numpy & Pandas have been imported for you.\n",
    "\n",
    "Your task now is to iterate over the list filenames and to use the function read_flights to build a list of delayed objects. Finally, you'll concatenate them into a Dask DataFrame with dd.from_delayed() and print out the mean of the WEATHER_DELAY column.\n",
    "\n",
    "Note: This exercise may take several seconds to execute."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filenames = ['flightdelays-2016-1.csv',\n",
    " 'flightdelays-2016-2.csv',\n",
    " 'flightdelays-2016-3.csv',\n",
    " 'flightdelays-2016-4.csv',\n",
    " 'flightdelays-2016-5.csv']\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Loop over filenames with index filename\n",
    "for filename in filenames:\n",
    "    # Apply read_flights to filename; append to dataframes\n",
    "    dataframes.append(read_flights('../datasets/flightdelays/flightdelays/' + filename))\n",
    "\n",
    "\n",
    "# Compute flight delays: flight_delays\n",
    "flight_delays = dd.from_delayed(dataframes)\n",
    "\n",
    "# Print average of 'WEATHER_DELAY' column of flight_delays\n",
    "print(flight_delays['WEATHER_DELAY'].mean().compute())"
   ]
  },
  {
   "source": [
    "# Deferring reading weather data\n",
    "For this exercise, daily weather data is provided from 2016 for 5 US cities: Atlanta, Denver, Dallas-Fort Worth, Orlando, and Chicago. The weather data comes from Weather Underground and is found in separate CSV files labelled by airport code (e.g., ATL.csv). The list filenames contains the names of these 5 files. The ultimate goal is to correlate the flight delays with weather events from each day of 2016.\n",
    "\n",
    "As with the flight-delays data, you'll need to clean the weather data as it is read in. Your job is to define a function that loads a DataFrame from a file, cleans the DataFrame's 'PrecipitationIn' column, and appends an 'Airport' column with the appropriate airport code for each record."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define @delayed-function read_weather with input filename\n",
    "@delayed\n",
    "def read_weather(filename):\n",
    "    # Read in filename: df\n",
    "    df = pd.read_csv(filename, parse_dates=['Date'])\n",
    "\n",
    "    # Clean 'PrecipitationIn'\n",
    "    df['PrecipitationIn'] = pd.to_numeric(df['PrecipitationIn'], errors='coerce')\n",
    "\n",
    "    # Create the 'Airport' column\n",
    "    df['Airport'] = filename.split('.')[0]\n",
    "\n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "source": [
    "# Building a weather DataFrame\n",
    "Your job now is to construct a Dask DataFrame using the function from the previous exercise. To do this, you will iterate over the list filenames provided and build up a list of delayed DataFrames. You'll then concatenate those delayed DataFrames into a Dask DataFrame with dd.from_delayed() as you did with the flight information. Finally, you'll print the row with largest 'Max TemperatureF' value.\n",
    "\n",
    "The list filenames contains the names of the CSV files of weather data labelled by airport code for Atlanta, Denver, Dallas-Fort Worth, Orlando, and Chicago. The read_weather function from the previous exercise is also provided for you and dask.dataframe is imported as dd. Additionally, an empty list called weather_dfs has been created for you."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['ATL.csv', 'DEN.csv', 'DFW.csv', 'MCO.csv', 'ORD.csv']\n",
    "weather_dfs = []\n",
    "# Loop over filenames with filename\n",
    "for filename in filenames:\n",
    "    # Invoke read_weather on filename; append result to weather_dfs\n",
    "    weather_dfs.append(read_weather('../datasets/weatherdata/' + filename))\n",
    "\n",
    "# Call dd.from_delayed() with weather_dfs: weather\n",
    "weather = dd.from_delayed(weather_dfs)\n",
    "\n",
    "# Print result of weather.nlargest(1, 'Max TemperatureF')\n",
    "print(weather.nlargest(1, 'Max TemperatureF').compute())"
   ]
  },
  {
   "source": [
    "# Which city gets the most snow?\n",
    "The Dask DataFrame weather from the previous exercise is provided here.\n",
    "\n",
    "Your task now is to aggregate the total snow fall for each airport (at least those airports that experienced snow). You'll use the method .str.contains() to create a boolean Series identifying snowy days. You'll need to chain with the method fillna(False) as well; this is to clean NaN values from the boolean Series so it can be used for selection within the .loc[] accessor. After filtering rows that correspond to snowy days from weather, you'll group the rows of the filtered DataFrame by airport code. This allows you to extract the precipitation column and compute aggregated sums grouped by airport."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make cleaned Boolean Series from weather['Events']: is_snowy\n",
    "is_snowy = weather['Events'].str.contains('Snow').fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered DataFrame with weather.loc & is_snowy: got_snow\n",
    "got_snow = weather.loc[is_snowy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby 'Airport' column; select 'PrecipitationIn'; aggregate sum(): result\n",
    "result = got_snow.groupby('Airport')['PrecipitationIn'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute & print the value of result\n",
    "print(result.compute())"
   ]
  },
  {
   "source": [
    "# Persisting merged DataFrame\n",
    "If your data can fit into memory, either on a workstation or a cluster, persisting the Dask DataFrame with the .persist() method can be useful. In particular, persisting DataFrames can provide significant speed-up because the time-consuming work of reading from disk is performed only once.\n",
    "\n",
    "The following function, which computes the percentage of flights delayed by weather, has been defined for you:\n",
    "\n",
    "```\n",
    "def percent_delayed(df):\n",
    "    return (df['WEATHER_DELAY'].count() / len(df)) * 100\n",
    "```\n",
    "Your job is to time the execution of that function using the weather_delays DataFrame. You'll then call .persist() on the DataFrame and compare execution times of the function with the persisted DataFrame.\n",
    "\n",
    "Note: This exercise will take several seconds to execute."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_delayed(df):\n",
    "    return (df['WEATHER_DELAY'].count() / len(df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "weather_delays = weather\n",
    "# Print time in milliseconds to compute percent_delayed on weather_delays\n",
    "t_start = time.time()\n",
    "print(percent_delayed(weather_delays).compute())\n",
    "t_end = time.time()\n",
    "print((t_end-t_start)*1000)\n",
    "\n",
    "# Call weather_delays.persist(): persisted_weather_delays\n",
    "persisted_weather_delays = weather_delays.persist()\n",
    "\n",
    "# Print time in milliseconds to compute percent_delayed on persisted_weather_delays\n",
    "t_start = time.time()\n",
    "print(percent_delayed(persisted_weather_delays).compute())\n",
    "t_end = time.time()\n",
    "print((t_end-t_start)*1000)"
   ]
  },
  {
   "source": [
    "# Finding sources of weather delays\n",
    "The final step in this case study is to use the persisted_weather_delays Dask DataFrame to determine the percentage of delayed flights per weather event.\n",
    "\n",
    "Your job is to compute the number of delayed flights by weather events and divide by the total number of flights. You'll then use .nlargest(5) to retrieve the five highest contributions to the number of delayed flights. Finally, you'll compute the average length of the delay for the 5 leading contributions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group persisted_weather_delays by 'Events': by_event\n",
    "by_event = persisted_weather_delays.groupby('Events')\n",
    "\n",
    "# Count 'by_event['WEATHER_DELAY'] column & divide by total number of delayed flights\n",
    "pct_delayed = by_event['WEATHER_DELAY'].count() / persisted_weather_delays['WEATHER_DELAY'].count() * 100\n",
    "\n",
    "# Compute & print five largest values of pct_delayed\n",
    "print(pct_delayed.nlargest(5).compute())\n",
    "\n",
    "# Calculate mean of by_event['WEATHER_DELAY'] column & return the 5 largest entries: avg_delay_time\n",
    "avg_delay_time = by_event['WEATHER_DELAY'].mean().nlargest(5)\n",
    "\n",
    "# Compute & print avg_delay_time\n",
    "print(avg_delay_time.compute())"
   ]
  }
 ]
}